{
  "$schema": "schema.json",
  "data": [
    {
      "name": "Geoffrey Hinton",
      "link": "https://www.cs.toronto.edu/~hinton/",
      "tagline": "University of Toronto - Godfather of AI",
      "rationale": "After decades of pioneering neural network research, Hinton resigned from Google in 2023 to speak freely about AI risks, warning that advanced AI could surpass human intelligence and potentially pose existential threats."
    },
    {
      "name": "Yoshua Bengio",
      "link": "https://yoshuabengio.org/",
      "tagline": "LawZero - MILA - Pioneer of Deep Learning",
      "rationale": "While continuing fundamental AI research at MILA, Bengio has shifted focus toward AI safety and governance, advocating for careful development and regulation of powerful AI systems to prevent potential harms."
    },
    {
      "name": "Stuart Russell",
      "tagline": "UC Berkeley - AI Safety Pioneer",
      "link": "https://people.eecs.berkeley.edu/~russell/",
      "rationale": "As a leading AI researcher at Berkeley, Russell has dedicated significant efforts to highlighting AI alignment problems, authoring 'Human Compatible' and advocating for development approaches that prioritize safety over capabilities."
    },
    {
      "name": "Roman Yampolskiy",
      "link": "https://engineering.louisville.edu/faculty/roman-v-yampolskiy/",
      "tagline": "University of Louisville - AI Safety Pioneer",
      "rationale": "Specializing in AI safety, Yampolskiy consistently warns about existential risks from advanced AI and argues that uncontrolled AI development could lead to catastrophic outcomes, advocating for pauses in frontier AI research."
    },
    {
      "name": "Nick Bostrom",
      "link": "https://www.nickbostrom.com/",
      "tagline": "FHI Oxford - AI Safety Pioneer",
      "rationale": "As founder of Oxford's Future of Humanity Institute, Bostrom has been instrumental in bringing attention to AI existential risks through his book 'Superintelligence' and advocates for careful AI governance."
    },
    {
      "name": "Max Tegmark",
      "link": "https://space.mit.edu/home/tegmark/",
      "tagline": "MIT - Future of Life Institute",
      "rationale": "MIT physicist and co-founder of the Future of Life Institute, Tegmark actively campaigns for AI safety measures and responsible development, having organized open letters calling for caution in AI advancement."
    },
    {
      "name": "Eliezer Yudkowsky",
      "link": "https://www.yudkowsky.net/",
      "tagline": "MIRI - Pioneer of AI Alignment Theory",
      "rationale": "Founder of MIRI, Yudkowsky has consistently warned about AI alignment problems and existential risks, advocating for complete moratoriums on advanced AI development until robust safety measures are established."
    },
    {
      "name": "Nate Soares",
      "link": "https://intelligence.org/team/",
      "tagline": "MIRI - Alignment Researcher",
      "rationale": "As executive director of MIRI, Soares focuses exclusively on addressing AI alignment problems and potential catastrophic risks from advanced AI rather than contributing to capability advancements."
    },
    {
      "name": "Paul Christiano",
      "link": "https://paulfchristiano.com/",
      "tagline": "Ex-OpenAI - US AI Safety Institute (AISI)",
      "rationale": "Founded the Alignment Research Center (also METR) after leaving OpenAI, focusing exclusively on AI alignment problems while abstaining from capabilities research that could accelerate AI development. Currently at US AI Safety Institute."
    },
    {
      "name": "Steven Adler",
      "link": "https://scholar.google.com/citations?user=K8mpmWAAAAAJ",
      "tagline": "Ex-OpenAI - AI Safety Whistleblower",
      "rationale": "Ex-OpenAI engineer who resigned over AI safety concerns, specifically citing his unwillingness to contribute to systems that might lead to unaligned artificial general intelligence."
    },
    {
      "name": "William Saunders",
      "link": "https://scholar.google.com/citations?user=8hjFFAoAAAAJ",
      "tagline": "Ex-OpenAI - ARC - Safety Researcher",
      "rationale": "Left OpenAI to join the Alignment Research Center, expressing concerns about the pace of AI development and choosing to focus exclusively on alignment research rather than capabilities advancement."
    },
    {
      "name": "Miles Brundage",
      "link": "https://www.milesbrundage.com/",
      "tagline": "AI Governance Expert - Responsible AI Advocate",
      "rationale": "Prominent AI policy researcher who has consistently advocated for responsible AI development and governance while warning about safety risks from advanced systems."
    },
    {
      "name": "Rosie Campbell",
      "link": "https://www.rosiecampbell.xyz/about",
      "tagline": "Ex-OpenAI - Ex-Anthropic - PAI",
      "rationale": "Left OpenAI to co-found the Anthropic safety team before moving to the Partnership on AI, consistently focusing her career on AI safety and governance rather than capabilities advancement."
    },
    {
      "name": "Gretchen Krueger",
      "link": "https://scholar.google.com/citations?user=astFxkwAAAAJ",
      "tagline": "Ex-OpenAI - AI Safety Advocate",
      "rationale": "Ex-researcher at OpenAI who shifted focus to AI safety and has spoken publicly about the importance of careful development of increasingly powerful AI systems."
    },
    {
      "name": "Cullen O'Keefe",
      "link": "https://www.cullenokeefe.com/",
      "tagline": "Ex-OpenAI - AI Governance Expert",
      "rationale": "Ex-legal counsel at OpenAI who left to focus on AI governance and safety through EA organizations, advocating for legal frameworks to mitigate AI risks rather than accelerating capabilities."
    },
    {
      "name": "Daniel Kokotajlo",
      "link": "https://www.alignmentforum.org/users/daniel-kokotajlo",
      "tagline": "Ex-OpenAI - AI Safety Researcher",
      "rationale": "Ex-OpenAI safety researcher who has consistently focused on existential risks from advanced AI, particularly timelines and forecasting risks, while avoiding work that could accelerate AI capability development."
    },
    {
      "name": "Dan Hendrycks",
      "link": "https://people.eecs.berkeley.edu/~hendrycks/",
      "tagline": "CAIS - AI Safety Pioneer & Benchmarks Creator",
      "rationale": "As founder of the Center for AI Safety, he focuses exclusively on research aimed at reducing catastrophic AI risks while developing safety benchmarks and advocating for responsible AI development over capabilities advancement."
    },
    {
      "name": "Oliver Zhang",
      "link": "https://safe.ai/about",
      "tagline": "CAIS - AI Safety Researcher & Policy Advocate",
      "rationale": "Researcher at the Center for AI Safety focused on catastrophic AI risk reduction, contributing to safety research and policy advocacy while abstaining from work that would accelerate AI capabilities development without adequate safety measures."
    },
    {
      "name": "Connor Leahy",
      "link": "https://en.wikipedia.org/wiki/Connor_Leahy",
      "tagline": "Conjecture AI - AI Safety Researcher",
      "rationale": "After co-founding EleutherAI, Leahy shifted focus to establishing Conjecture, an organization dedicated to solving AI alignment problems while advocating for safety-first approaches to AI development and openly warning about existential risks. Also co-founded ControlAI."
    },
    {
      "name": "Sid Black",
      "link": "https://scholar.google.com/citations?user=ckPrWskAAAAJ",
      "tagline": "Conjecture AI - AI Safety Researcher",
      "rationale": "Co-founder of EleutherAI who has pivoted to focus on AI safety concerns, advocating for responsible development practices and emphasizing the importance of alignment research over capabilities advancement."
    },
    {
      "name": "Gabriel Alfour",
      "link": "https://www.conjecture.dev/about",
      "tagline": "Conjecture AI - ML Ethics Researcher",
      "rationale": "Researcher focused on AI ethics and risk reduction who has consistently raised concerns about uncontrolled AI advancement and advocated for stronger safety measures before further capabilities development."
    },
    {
      "name": "Jacob Hilton",
      "link": "https://www.jacobh.co.uk/",
      "tagline": "Ex-OpenAI Research Scientist - ARC",
      "rationale": "Left his position at OpenAI to join the Alignment Research Center, dedicating his work exclusively to AI alignment problems and safety research rather than continuing to advance AI capabilities at a frontier lab."
    },
    {
      "name": "Richard Ngo",
      "link": "https://scholar.google.com/citations?user=7CY93A4AAAAJ",
      "tagline": "Ex-OpenAI - AI Safety Researcher ",
      "rationale": "Former OpenAI governance team member who publicly resigned over concerns that the company had departed from its mission of ensuring AGI development goes well, previously worked on safety at DeepMind and has extensively published on AI alignment challenges."
    },
    {
      "name": "Jesse Hoogland",
      "link": "https://www.jessehoogland.com/",
      "tagline": "Timaeus - AI Safety Researcher",
      "rationale": "Leads Timaeus, an AI safety research organization focused on developmental interpretability, with a strong emphasis on singular learning theory to understand how AI systems learn and generalize, advocating for safer AI development through rigorous empirical and theoretical research."
    },
    {
      "name": "Daniel Murfet",
      "link": "https://scholar.google.com/citations?user=ilWvLq0AAAAJ",
      "tagline": "Timaeus - U of Melbourne - AI Safety Mathematician",
      "rationale": "University of Melbourne mathematician and research director at Timaeus who has shifted focus to AI safety research, applying mathematical foundations to understanding AI alignment problems and advocating for rigorous safety approaches before advancing AI capabilities."
    },
    {
      "name": "Alexander Gietelink Oldenziel",
      "link": "https://openreview.net/profile?id=~Alexander_Gietelink_Oldenziel1",
      "tagline": "Timaeus - AI Safety Researcher",
      "rationale": "Researcher at Timaeus focused on AI safety and alignment problems, contributing to the rationalist community's understanding of AI risks while advocating for safety-first approaches to AI development over capabilities advancement."
    },
    {
      "name": "Jeffrey Ladish",
      "link": "https://jeffreyladish.com/",
      "tagline": "Palisade Research - AI Safety & Cybersecurity",
      "rationale": "Leads Palisade Research, focusing on offensive AI capabilities to understand risks of losing control to AI systems, with prior experience building Anthropic’s information security program and advising government on AI and emerging tech risks."
    },
    {
      "name": "Sean O hEigeartaigh",
      "link": "https://www.cser.ac.uk/team/sean-o-heigeartaigh/",
      "tagline": "University of Cambridge - CSER",
      "rationale": "Leads the AI: Futures and Responsibility Programme at the University of Cambridge, focusing on foresight, risk, and governance of advanced AI systems, with a strong emphasis on mitigating catastrophic risks. His work at the Centre for the Study of Existential Risk (CSER) prioritizes AI safety without contributing to capabilities research, and he has advised global bodies like the OECD and UN on AI risks."
    },
    {
      "name": "Ryan Greenblatt",
      "link": "https://www.redwoodresearch.org/team",
      "tagline": "Redwood Research - AI Alignment Researcher",
      "rationale": "Researcher at Redwood Research focusing on AI safety and alignment problems, working specifically on methods to make AI systems safer rather than more capable, with emphasis on reducing risks from advanced AI development."
    },
    {
      "name": "Buck Shlegeris",
      "link": "https://www.redwoodresearch.org/team",
      "tagline": "Redwood Research - AI Safety Researcher",
      "rationale": "Redwood Research scientist focused exclusively on AI alignment problems who has consistently advocated against advancing AI capabilities without robust safety measures, dedicating his work to reducing existential risks from advanced AI systems."
    },
    {
      "name": "Adam Gleave",
      "link": "https://far.ai/about/team",
      "tagline": "FAR.AI - AI Safety & Alignment Researcher",
      "rationale": "Founder of FAR.AI dedicated to fundamental AI alignment research, focusing exclusively on safety problems and risk reduction rather than contributing to capabilities advancement that could accelerate unsafe AI development."
    },
    {
      "name": "Karl Berzins",
      "link": "https://far.ai/about/team",
      "tagline": "FAR.AI - AI Alignment Researcher",
      "rationale": "Research scientist at FAR.AI working on AI alignment and safety problems, dedicating his efforts to reducing existential risks from advanced AI systems while avoiding work that would accelerate capabilities without safety guarantees."
    },
    {
      "name": "David Scott Krueger",
      "link": "https://davidscottkrueger.com/",
      "tagline": "Cambridge - AI Safety & Alignment Researcher",
      "rationale": "University of Cambridge researcher focused on AI safety and alignment problems, advocating for careful development of AI systems and emphasizing safety research over capabilities advancement to reduce potential existential risks."
    },
    {
      "name": "Jacob Steinhardt",
      "link": "https://jsteinhardt.stat.berkeley.edu/",
      "tagline": "UC Berkeley - AI Safety & Robustness Researcher",
      "rationale": "UC Berkeley professor specializing in AI safety and robustness research, focusing on making AI systems more reliable and safer rather than advancing capabilities, with particular emphasis on understanding and mitigating AI risks."
    },
    {
      "name": "Beth Barnes",
      "link": "https://metr.org/team/beth-barnes/",
      "tagline": "METR - Frontier AI Safety Researcher",
      "rationale": "Research scientist at METR (formerly ARC Evals) working on evaluating and understanding risks from frontier AI systems, focusing on safety assessment and risk mitigation rather than advancing AI capabilities."
    },
    {
      "name": "Marius Hobbhahn",
      "link": "https://www.mariushobbhahn.com/",
      "tagline": "Apollo Research - AI Safety & Alignment Researcher",
      "rationale": "CEO and co-founder of Apollo Research, an organization dedicated to AI safety and alignment research, focusing on reducing existential risks from advanced AI systems while advocating for safety-first approaches over rapid capabilities development."
    },
    {
      "name": "Owain Evans",
      "link": "https://www.truthfulai.org/about",
      "tagline": "Truthful AI - CHAI - AI Safety Researcher",
      "rationale": "Founder of Truthful AI focused on developing methods to make AI systems more truthful and reliable, prioritizing AI safety and alignment research over capabilities advancement to reduce risks from deceptive or unaligned AI systems."
    },
    {
      "name": "Tamsin Leake",
      "link": "https://orxl.org/",
      "tagline": "Orthogonal - AI Alignment Organization Researcher",
      "rationale": "Researcher at Orthogonal, an organization focused specifically on AI alignment problems, dedicating her work to ensuring AI systems remain aligned with human values while avoiding contributions to capabilities advancement."
    },
    {
      "name": "Stuart Armstrong",
      "link": "https://buildaligned.ai/about-us",
      "tagline": "Aligned AI - AI Safety Research Pioneer",
      "rationale": "Co-founder of Aligned AI and former FHI researcher, dedicated exclusively to AI alignment and safety problems, advocating for careful AI development approaches while abstaining from work that would advance AI capabilities without safety guarantees."
    },
    {
      "name": "Rebecca Gorman",
      "link": "https://buildaligned.ai/about-us",
      "tagline": "Aligned AI - AI Safety & Alignment Researcher",
      "rationale": "Co-founder of Aligned AI, an organization dedicated exclusively to AI alignment and safety research, focusing on developing approaches to ensure AI systems remain beneficial while abstaining from capabilities research that could accelerate unsafe AI development."
    },
    {
      "name": "John Wentworth",
      "link": "https://www.alignmentforum.org/users/johnswentworth",
      "tagline": "Independent AI Alignment Theorist",
      "rationale": "Independent researcher focused exclusively on fundamental AI alignment theory and problems, contributing to alignment research through theoretical work while completely abstaining from any capabilities research or development."
    },
    {
      "name": "David Manheim",
      "link": "https://about.me/davidmanheim",
      "tagline": "ALTER - AI Safety & Risk Analysis Expert",
      "rationale": "Researcher at ALTER focused on AI safety and existential risk analysis, dedicating his work to understanding and preventing catastrophic outcomes from advanced AI while abstaining from capabilities research."
    },
    {
      "name": "Vanessa Kosoy",
      "link": "https://alter.org.il/en/about-alter-2/",
      "tagline": "ALTER - AGI Alignment Researcher",
      "rationale": "AGI alignment researcher at ALTER focused exclusively on solving alignment problems for artificial general intelligence, working on theoretical foundations to ensure safe AI development while avoiding contributions to capabilities advancement."
    },
    {
      "name": "Shahar Avin",
      "link": "https://www.shaharavin.com/",
      "tagline": "CSER Cambridge - AI Risk & Safety Researcher",
      "rationale": "Senior Research Associate at Cambridge's Centre for the Study of Existential Risk, focusing on understanding and mitigating catastrophic risks from advanced AI systems while advocating for safety-first approaches over capabilities advancement."
    },
    {
      "name": "Dylan Hadfield-Menell",
      "link": "https://people.csail.mit.edu/dhm/",
      "tagline": "MIT CSAIL - Algorithmic Alignment Group",
      "rationale": "MIT professor who runs the Algorithmic Alignment Group, focusing exclusively on AI alignment problems and ensuring AI systems align with human values and societal goals, while advocating for AI safety approaches over pure capabilities advancement."
    },
    {
      "name": "Douglas Hofstadter",
      "link": "https://www.youtube.com/watch?v=lfXxzAVtdpU&t=1763s",
      "tagline": "Indiana University - Cognitive Scientist & Author",
      "rationale": "Renowned cognitive scientist and author of 'Gödel, Escher, Bach'. He significantly shifted his views on AI timelines, stated that the progress of AI is so unexpected and accelerating that it feels like an oncoming tsunami that will catch humanity off guard. Historically he's been an AI skeptic, advocated for careful consideration of consciousness and intelligence rather than pursuing advanced AI capabilities without understanding."
    },
    {
      "name": "Carl Feynman",
      "link": "https://independent.academia.edu/CarlFeynman",
      "tagline": "AI Safety Advocate & Computer Scientist",
      "rationale": "Computer scientist and son of physicist Richard Feynman who has advocated for careful AI development approaches, expressing concerns about advanced AI systems and supporting safety-first methodologies over rapid capabilities advancement."
    },
    {
      "name": "Esben Kran",
      "link": "https://kran.ai/cache/a439a79eb419be5dda635034f722ec41",
      "tagline": "Apart Research CEO - AI Safety Researcher",
      "rationale": "CEO and co-founder of Apart Research, an organization dedicated to AI safety research and alignment problems, focusing on reducing existential risks from advanced AI systems while prioritizing safety measures over rapid capabilities development."
    },
    {
      "name": "Jason Hoelscher-Obermaier",
      "link": "https://www.linkedin.com/in/jason-hoelscher-obermaier/",
      "tagline": "Apart Research - AI Safety Research Director",
      "rationale": "Research director at Apart Research focused on AI safety and alignment problems, leading initiatives to understand and mitigate risks from advanced AI systems while advocating for safety-first approaches over capabilities advancement."
    },
    {
      "name": "Paul Riechers",
      "link": "https://www.linkedin.com/in/paul-riechers/",
      "tagline": "Simplex - AI Safety & Complexity Researcher",
      "rationale": "Researcher at Simplex focused on AI safety through complexity science approaches, working to understand and mitigate risks from advanced AI systems while abstaining from capabilities research that could accelerate unsafe AI development."
    },
    {
      "name": "Adam Shai",
      "link": "https://www.linkedin.com/in/adam-shai/",
      "tagline": "Simplex - AI Safety & Neuroscience Researcher",
      "rationale": "Researcher at Simplex applying neuroscience principles to AI safety problems, focusing on understanding AI system behavior to ensure safe development while avoiding work that would advance capabilities without safety guarantees."
    },
    {
      "name": "Steve Byrnes",
      "link": "https://www.stevebyrnes.com/",
      "tagline": "Astera Institute - AI Safety & Neuroscience Researcher",
      "rationale": "Physics PhD from UC Berkeley who transitioned from physics research to dedicated AI safety work at Astera Institute. With a 90% P(Doom) assessment, he focuses specifically on preventing catastrophic outcomes from advanced AI systems, particularly brain-like AGI safety challenges at the intersection of neuroscience and AI alignment."
    },
    {
      "name": "Ajeya Cotra",
      "link": "https://www.alignmentforum.org/users/ajeya-cotra",
      "tagline": "Open Philanthropy - AI Forecasting & Safety Research",
      "rationale": "Senior researcher leading technical AI safety grantmaking and forecasting at Open Philanthropy. Her influential work on AI timelines and capability development explicitly models catastrophic risks from advanced AI systems, focusing funding on preventing AI takeover scenarios and loss of human control."
    },
    {
      "name": "Jess Whittlestone",
      "link": "https://www.jesswhittlestone.com/",
      "tagline": "CLTR - AI Governance & Safety",
      "rationale": "Senior researcher at the Centre for Long-Term Resilience focusing on long-term AI governance and safety challenges. Her work emphasizes the catastrophic risks from advanced AI systems and the need for robust governance frameworks to prevent existential threats from uncontrolled AI development."
    },
    {
      "name": "Benjamin Weinstein-Raun",
      "link": "https://www.linkedin.com/in/benjamin-weinstein-raun/",
      "tagline": "Palisade Research - AI Safety & Risk Analysis",
      "rationale": "Researcher at Palisade Research working on AI safety and cybersecurity risks, focusing on understanding scenarios where humanity could lose control to AI systems. His work at Palisade contributes to analyzing and preventing catastrophic outcomes from advanced AI development."
    }
  ]
}
