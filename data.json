{
  "$schema": "schema.json",
  "data": [
    {
      "name": "Geoffrey Hinton",
      "link": "https://www.cs.toronto.edu/~hinton/",
      "tagline": "University of Toronto - Godfather of AI",
      "rationale": "After decades of pioneering neural network research, Hinton resigned from Google in 2023 to speak freely about AI risks, warning that advanced AI could surpass human intelligence and potentially pose existential threats."
    },
    {
      "name": "Yoshua Bengio",
      "link": "https://yoshuabengio.org/",
      "tagline": "MILA - Pioneer of Deep Learning",
      "rationale": "While continuing fundamental AI research at MILA, Bengio has shifted focus toward AI safety and governance, advocating for careful development and regulation of powerful AI systems to prevent potential harms."
    },
    {
      "name": "Stuart Russell",
      "tagline": "UC Berkeley - AI Safety Pioneer",
      "link": "https://people.eecs.berkeley.edu/~russell/",
      "rationale": "As a leading AI researcher at Berkeley, Russell has dedicated significant efforts to highlighting AI alignment problems, authoring 'Human Compatible' and advocating for development approaches that prioritize safety over capabilities."
    },
    {
      "name": "Roman Yampolskiy",
      "link": "https://engineering.louisville.edu/faculty/roman-v-yampolskiy/",
      "tagline": "University of Louisville - AI Safety Pioneer",
      "rationale": "Specializing in AI safety, Yampolskiy consistently warns about existential risks from advanced AI and argues that uncontrolled AI development could lead to catastrophic outcomes, advocating for pauses in frontier AI research."
    },
    {
      "name": "Nick Bostrom",
      "link": "https://www.nickbostrom.com/",
      "tagline": "FHI Oxford - AI Safety Pioneer",
      "rationale": "As founder of Oxford's Future of Humanity Institute, Bostrom has been instrumental in bringing attention to AI existential risks through his book 'Superintelligence' and advocates for careful AI governance."
    },
    {
      "name": "Max Tegmark",
      "link": "https://space.mit.edu/home/tegmark/",
      "tagline": "MIT - Future of Life Institute",
      "rationale": "MIT physicist and co-founder of the Future of Life Institute, Tegmark actively campaigns for AI safety measures and responsible development, having organized open letters calling for caution in AI advancement."
    },
    {
      "name": "Eliezer Yudkowsky",
      "link": "https://www.yudkowsky.net/",
      "tagline": "MIRI - Pioneer of AI Alignment Theory",
      "rationale": "Founder of MIRI, Yudkowsky has consistently warned about AI alignment problems and existential risks, advocating for complete moratoriums on advanced AI development until robust safety measures are established."
    },
    {
      "name": "Nate Soares",
      "link": "https://intelligence.org/team/",
      "tagline": "MIRI - Alignment Researcher",
      "rationale": "As executive director of MIRI, Soares focuses exclusively on addressing AI alignment problems and potential catastrophic risks from advanced AI rather than contributing to capability advancements."
    },
    {
      "name": "Paul Christiano",
      "link": "https://paulfchristiano.com/",
      "tagline": "Ex-OpenAI - US AI Safety Institute",
      "rationale": "Founded the Alignment Research Center (also METR) after leaving OpenAI, focusing exclusively on AI alignment problems while abstaining from capabilities research that could accelerate AI development. Currently at US AI Safety Institute."
    },
    {
      "name": "Steven Adler",
      "link": "https://scholar.google.com/citations?user=K8mpmWAAAAAJ",
      "tagline": "Ex-OpenAI - AI Safety Whistleblower",
      "rationale": "Ex-OpenAI engineer who resigned over AI safety concerns, specifically citing his unwillingness to contribute to systems that might lead to unaligned artificial general intelligence."
    },
    {
      "name": "William Saunders",
      "link": "https://scholar.google.com/citations?user=8hjFFAoAAAAJ",
      "tagline": "Ex-OpenAI - Safety Researcher - ARC",
      "rationale": "Left OpenAI to join the Alignment Research Center, expressing concerns about the pace of AI development and choosing to focus exclusively on alignment research rather than capabilities advancement."
    },
    {
      "name": "Miles Brundage",
      "link": "https://www.milesbrundage.com/",
      "tagline": "AI Governance Expert - Responsible AI Advocate",
      "rationale": "Prominent AI policy researcher who has consistently advocated for responsible AI development and governance while warning about safety risks from advanced systems."
    },
    {
      "name": "Rosie Campbell",
      "link": "https://www.rosiecampbell.xyz/about",
      "tagline": "Ex-OpenAI & Anthropic",
      "rationale": "Left OpenAI to co-found the Anthropic safety team before moving to the Partnership on AI, consistently focusing her career on AI safety and governance rather than capabilities advancement."
    },
    {
      "name": "Gretchen Krueger",
      "link": "https://scholar.google.com/citations?user=astFxkwAAAAJ",
      "tagline": "Ex-OpenAI - AI Safety Advocate",
      "rationale": "Ex-researcher at OpenAI who shifted focus to AI safety and has spoken publicly about the importance of careful development of increasingly powerful AI systems."
    },
    {
      "name": "Cullen O'Keefe",
      "link": "https://www.cullenokeefe.com/",
      "tagline": "Ex-OpenAI - AI Governance Expert",
      "rationale": "Ex-legal counsel at OpenAI who left to focus on AI governance and safety through EA organizations, advocating for legal frameworks to mitigate AI risks rather than accelerating capabilities."
    },
    {
      "name": "Daniel Kokotajlo",
      "link": "https://www.alignmentforum.org/users/daniel-kokotajlo",
      "tagline": "Ex-OpenAI - Safety-Focused Researcher",
      "rationale": "Ex-OpenAI safety researcher who has consistently focused on existential risks from advanced AI, particularly timelines and forecasting risks, while avoiding work that could accelerate AI capability development."
    },
    {
      "name": "Dan Hendrycks",
      "link": "https://people.eecs.berkeley.edu/~hendrycks/",
      "tagline": "CAIS - AI Safety Pioneer & Benchmarks Creator",
      "rationale": "As founder of the Center for AI Safety, he focuses exclusively on research aimed at reducing catastrophic AI risks while developing safety benchmarks and advocating for responsible AI development over capabilities advancement."
    },
    {
      "name": "Oliver Zhang",
      "link": "https://safe.ai/about",
      "tagline": "CAIS - AI Safety Researcher & Policy Advocate",
      "rationale": "Researcher at the Center for AI Safety focused on catastrophic AI risk reduction, contributing to safety research and policy advocacy while abstaining from work that would accelerate AI capabilities development without adequate safety measures."
    },
    {
      "name": "Connor Leahy",
      "link": "https://en.wikipedia.org/wiki/Connor_Leahy",
      "tagline": "Conjecture AI - AI Safety Researcher",
      "rationale": "After co-founding EleutherAI, Leahy shifted focus to establishing Conjecture, an organization dedicated to solving AI alignment problems while advocating for safety-first approaches to AI development and openly warning about existential risks. Also co-founded ControlAI."
    },
    {
      "name": "Sid Black",
      "link": "https://scholar.google.com/citations?user=ckPrWskAAAAJ",
      "tagline": "Conjecture AI - AI Safety Researcher",
      "rationale": "Co-founder of EleutherAI who has pivoted to focus on AI safety concerns, advocating for responsible development practices and emphasizing the importance of alignment research over capabilities advancement."
    },
    {
      "name": "Gabriel Alfour",
      "link": "https://www.conjecture.dev/about",
      "tagline": "Conjecture AI - ML Ethics Researcher",
      "rationale": "Researcher focused on AI ethics and risk reduction who has consistently raised concerns about uncontrolled AI advancement and advocated for stronger safety measures before further capabilities development."
    },
    {
      "name": "Jacob Hilton",
      "link": "https://www.jacobh.co.uk/",
      "tagline": "Ex-OpenAI Research Scientist - ARC",
      "rationale": "Left his position at OpenAI to join the Alignment Research Center, dedicating his work exclusively to AI alignment problems and safety research rather than continuing to advance AI capabilities at a frontier lab."
    },
    {
      "name": "Richard Ngo",
      "link": "https://scholar.google.com/citations?user=7CY93A4AAAAJ",
      "tagline": "Ex-OpenAI - AI Safety Researcher ",
      "rationale": "Former OpenAI governance team member who publicly resigned over concerns that the company had departed from its mission of ensuring AGI development goes well, previously worked on safety at DeepMind and has extensively published on AI alignment challenges."
    },
    {
      "name": "Jesse Hoogland",
      "link": "https://www.jessehoogland.com/",
      "tagline": "Timaeus - AI Safety Researcher",
      "rationale": "Leads Timaeus, an AI safety research organization focused on developmental interpretability, with a strong emphasis on singular learning theory to understand how AI systems learn and generalize, advocating for safer AI development through rigorous empirical and theoretical research."
    },
    {
     "name": "Daniel Murfet",
     "link": "https://scholar.google.com/citations?user=ilWvLq0AAAAJ",
     "tagline": "Timaeus - Melbourne Uni - AI Safety Mathematician",
     "rationale": "University of Melbourne mathematician and research director at Timaeus who has shifted focus to AI safety research, applying mathematical foundations to understanding AI alignment problems and advocating for rigorous safety approaches before advancing AI capabilities."
    },
    {
     "name": "Alexander Gietelink Oldenziel",
     "link": "https://openreview.net/profile?id=~Alexander_Gietelink_Oldenziel1",
     "tagline": "Timaeus - AI Safety Researcher",
     "rationale": "Researcher at Timaeus focused on AI safety and alignment problems, contributing to the rationalist community's understanding of AI risks while advocating for safety-first approaches to AI development over capabilities advancement."
    },
    {
      "name": "Jeffrey Ladish",
      "link": "https://jeffreyladish.com/",
      "tagline": "Palisade Research - AI Safety and Cybersecurity",
      "rationale": "Leads Palisade Research, focusing on offensive AI capabilities to understand risks of losing control to AI systems, with prior experience building Anthropic’s information security program and advising government on AI and emerging tech risks."
    },
    {
      "name": "Sean O hEigeartaigh",
      "link": "https://www.cser.ac.uk/team/sean-o-heigeartaigh/",
      "tagline": "University of Cambridge - CSER",
      "rationale": "Leads the AI: Futures and Responsibility Programme at the University of Cambridge, focusing on foresight, risk, and governance of advanced AI systems, with a strong emphasis on mitigating catastrophic risks. His work at the Centre for the Study of Existential Risk (CSER) prioritizes AI safety without contributing to capabilities research, and he has advised global bodies like the OECD and UN on AI risks."
    },
    {
      "name": "Ryan Greenblatt",
      "link": "https://www.redwoodresearch.org/team",
      "tagline": "Redwood Research - AI Alignment Researcher",
      "rationale": "Researcher at Redwood Research focusing on AI safety and alignment problems, working specifically on methods to make AI systems safer rather than more capable, with emphasis on reducing risks from advanced AI development."
    },
    {
      "name": "Buck Shlegeris",
      "link": "https://www.redwoodresearch.org/team",
      "tagline": "Redwood Research - AI Safety Researcher & Educator",
      "rationale": "Redwood Research scientist focused exclusively on AI alignment problems who has consistently advocated against advancing AI capabilities without robust safety measures, dedicating his work to reducing existential risks from advanced AI systems."
    },
    {
      "name": "Adam Gleave",
      "link": "https://far.ai/about/team",
      "tagline": "FAR.AI - AI Safety & Alignment Researcher",
      "rationale": "Founder of FAR.AI dedicated to fundamental AI alignment research, focusing exclusively on safety problems and risk reduction rather than contributing to capabilities advancement that could accelerate unsafe AI development."
    },
    {
      "name": "Karl Berzins",
      "link": "https://far.ai/about/team",
      "tagline": "FAR.AI - AI Alignment Research Scientist",
      "rationale": "Research scientist at FAR.AI working on AI alignment and safety problems, dedicating his efforts to reducing existential risks from advanced AI systems while avoiding work that would accelerate capabilities without safety guarantees."
    },
    {
      "name": "David Scott Krueger",
      "link": "https://davidscottkrueger.com/",
      "tagline": "Cambridge - AI Safety & Alignment Researcher",
      "rationale": "University of Cambridge researcher focused on AI safety and alignment problems, advocating for careful development of AI systems and emphasizing safety research over capabilities advancement to reduce potential existential risks."
    },
    {
      "name": "Jacob Steinhardt",
      "link": "https://jsteinhardt.stat.berkeley.edu/",
      "tagline": "UC Berkeley - AI Safety & Robustness Researcher",
      "rationale": "UC Berkeley professor specializing in AI safety and robustness research, focusing on making AI systems more reliable and safer rather than advancing capabilities, with particular emphasis on understanding and mitigating AI risks."
    },
    {
      "name": "Beth Barnes",
      "link": "https://metr.org/team/beth-barnes/",
      "tagline": "METR - Frontier AI Safety Researcher",
      "rationale": "Research scientist at METR (formerly ARC Evals) working on evaluating and understanding risks from frontier AI systems, focusing on safety assessment and risk mitigation rather than advancing AI capabilities."
    },
    {
     "name": "Marius Hobbhahn",
     "link": "https://www.mariushobbhahn.com/",
     "tagline": "Apollo Research CEO - AI Safety & Alignment Researcher",
     "rationale": "CEO and co-founder of Apollo Research, an organization dedicated to AI safety and alignment research, focusing on reducing existential risks from advanced AI systems while advocating for safety-first approaches over rapid capabilities development."
    },
    {
     "name": "Owain Evans",
     "link": "https://www.truthfulai.org/about",
     "tagline": "Truthful AI - CHAI - AI Safety Researcher",
     "rationale": "Founder of Truthful AI focused on developing methods to make AI systems more truthful and reliable, prioritizing AI safety and alignment research over capabilities advancement to reduce risks from deceptive or unaligned AI systems."
    }
  ]
}
